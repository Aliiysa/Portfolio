# Portfolio


## Creating Transformer Architecture from scratch. [Click here for view the file](https://github.com/Aliiysa/Portfolio/blob/main/Transformer_Network.ipynb)

![Transformer Architecture]([https://github.com/Aliiysa/Portfolio/blob/main/images_transformers/transformer.png](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/The-Transformer-model-architecture.png/800px-The-Transformer-model-architecture.png))

while Language Models became hot topic in 2020s it is important to know details about them. The Transformer architecture is a crucial and powerful building block in the field of natural language processing (NLP) and deep learning. It is the foundation for many of the state-of-the-art language models used today, including BERT, GPT-3, and T5. The Transformer architecture enables these models to effectively model and understand the complex relationships between words and phrases in natural language, and has revolutionized the field of NLP. By understanding the Transformer architecture, practitioners and researchers can better design, build, and optimize language models. In addition, the Transformer architecture is also being used in other fields, such as computer vision, and has the potential to impact a wide range of industries in the future.

In this project you can see detailed diagrams about **Transformer Architecture**, mathematical explanation of **Attention Mechanism** and overall view of **Encoder-Decoder** mechanism.
*Implemented with TensorFlow*.


## Creating U-Net Architecture from scratch. [Click here for view the file](https://github.com/Aliiysa/Portfolio/blob/main/U_Net.ipynb)

If you ever used Stable Diffusion for image genereation then you already have contacted with **U-Net Architecture**

The U-Net architecture is a popular and effective deep learning architecture for image generation, specifically for image segmentation tasks. The U-Net architecture is composed of an **Encoder** and **Decoder** network that are connected by a bottleneck layer. The encoder network is a series of convolutional layers that gradually downsample the input image, which helps capture and abstract the high-level features of the image. The decoder network is a series of upsampling layers that reconstructs the original size of the image, while also refining the segmentation mask. The bottleneck layer in the middle serves as a bridge between the encoder and decoder networks, and enables the model to effectively capture both local and global features of the image. Additionally, skip connections between the encoder and decoder networks help to preserve the fine details of the image during the downsampling and upsampling process, improving the accuracy of the segmentation mask.

In this project you can see detailed information about **Downsampling**, **Upsampling** and **Bottleneck layers**.
*Implemented with TensorFlow*.


